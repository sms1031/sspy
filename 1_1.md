https://www.bilibili.com/video/BV18142187g5/ InternLM2 全链路开源体系

开源模型InternLM 2024年1月17日，InternLM 2 已经开源，面向不同的使用需求，每个规格包含三个版本。 开源了1.8B、7B和20B，在主观和客观评估中都表现出色，发布了不同阶段的模型，分析了SFT和RLHF训练后的变化 训练框架InternEvo 模型在预训练、SFT 、 RLHF 中使用的训练框架为 InternEvo。 模型使用高效的轻量级预训练框架InternEvo进行模型训练，框架使我们能够在数千个GPU上扩展模型训练， 通过数据、张量、序列和管道并行技术来实现，为了进一步提高GPU内存效率InternEvo集成了各种Zero Redundancy Optimizer策略， 显著减少了训练所需的内存占用。为了提高硬件利用率，模型引入了FlashAttention技术和混合精度训练，使用BF16 OpenCompas 评测体系 一个支持多种大模型和数据集的评测平台，包括 LLaMA、LLaMa2、ChatGLM2、ChatGPT、Claude 等。 它支持超过50个数据集的评测。OpenCompass 使用 OpenMMLab 风格的配置文件来管理模型和数据集， 用户只需添加简单的配置就可以快速开始评测。此外， 它还支持通过 Python API 来评测使用 TurboMind 推理引擎加速的大模型 LMDeploy 一个高效且友好的 LLMs 模型部署工具箱，其功能涵盖了量化、推理和服务。LMDeploy 工具箱提供了以下核心功能： 高效的推理：开发了 Persistent Batch（即 Continuous Batch）、Blocked K/V Cache、动态拆分和融合、张量并行、高效的计算 kernel 等重要特性。 可靠的量化：支持权重量化和 k/v 量化。4bit 模型推理效率是 FP16 下的 2.4 倍。 便捷的服务：通过请求分发服务，支持多模型在多机、多卡上的推理服务。 有状态推理：通过缓存多轮对话过程中的 attention 的 k/v，记住对话历史，从而避免重复处理历史会话，显著提升长文本多轮对话场景中的效率
